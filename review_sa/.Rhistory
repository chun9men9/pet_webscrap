aggr(sleep)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
?cor
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
missing.data
install.packages('caret')
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = "medianImpute")
class(pre)
missing.data
pre = preProcess(missing.data, method = "medianImpute")
class(pre)
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
install.packages('Hmisc')
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
install.packages('RANN')
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
imputed.1nn
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
imputed.5nn
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
imputed.5nn
?impute
?predict
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd, na.rm=TRUE) #Standard deviations for the sleep dataset; any issues?
aggr(sleep) #A graphical interpretation of the missing values and their
#combinations within the dataset.
library(mice) #Load the multivariate imputation by chained equations library.
md.pattern(sleep) #Can also view this information from a data perspective.
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#sepal measurements and the flower species.
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the c
install.packages('deldir')
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
table(iris$Species, iris.imputed1NN$Species)
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#sepal measurements and the flower species.
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
#mark the missing values.
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
iris.imputed12NN = kNN(iris.example, k = 12)
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
install.packages('kknn')
help(cars)
help(cars)
cars #Investigating the cars dataset.
#Basic numerical EDA for cars dataset.
summary(cars) #Five number summaries.
sapply(cars, sd)
cor(cars)
hist(cars$speed, xlab = "Speed in MPH", main = "Histogram of Speed")
hist(cars$dist, xlab = "Distance in Feet", main = "Histogram of Distance")
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2)
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
residuals = cars$dist - (beta0 + beta1*cars$speed)
#Note the sum of the residuals is 0.
sum(residuals)
?segments
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Web Scraping Project/review_sa")
library(tidyverse)
library(tidytext)
library(stringr)
library(dplyr)
#filecsv='one_pos_review.csv'
#filecsv='one_neg_review.csv'
filecsv='posneg_review.csv'
texts=read.csv(filecsv,header=TRUE, stringsAsFactors = FALSE)
head(texts)
texts$review_num=as.factor(texts$review_num)
### Tag Line Number ###
texts2 <- texts %>%
#group_by(book) %>%
mutate(linenumber = row_number())%>%
ungroup()
texts2
### Tokenize each word (separate each word to a Single Row)
library(tidytext)
texts3 <- texts2 %>%
unnest_tokens(word, review_text)
texts3
###Remove Stop Words
data("stop_words")
cleaned_texts <- texts3 %>%
anti_join(stop_words)
cleaned_texts %>%
count(word, sort = TRUE)
###Retrieve 'Joy' sentiments and store it in nrcjoy
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
### List out Words associated with Joy and their Counts
#cleaned_texts %>%
#  semi_join(nrcjoy) %>%
#  count(word, sort = TRUE)
### Find a sentiment score for each word using the Bing lexicon
library(tidyr)
bing <- get_sentiments("bing")
review_sentiment <- cleaned_texts %>%
inner_join(bing) %>%
count(review_num, index = linenumber,
sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
### Finally, plot the trajectory of sentiments as we go along each word
library(ggplot2)
ggplot(review_sentiment, aes(index,sentiment,fill=(sentiment>0))) +
geom_bar(stat = "identity", show.legend = FALSE) +
xlab('Review Number') +
ylab('Sentiment') +
facet_wrap(~review_num, ncol = 2, scales = "free_x") +
theme_dark() +
ggtitle('Sentiment Analysis on Negative and Positive Reviews')
#word_tb <- texts%>%unnest_tokens(word, TXT)
#}
#run_sa('neg_review.csv')
#ggplot(cleaned_texts, aes(index,sentiment,fill=sentiment)) +
#  geom_bar(stat = "identity", show.legend = FALSE)
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Web Scraping Project/review_sa")
library(tidyverse)
library(tidytext)
library(stringr)
library(dplyr)
#run_sa=function(filecsv, cloudname,excludelis=c('')){
filecsv='one_pos_review.csv'
texts=read.csv(filecsv,header = FALSE, stringsAsFactors = FALSE)
# head(texts)
# texts=texts[,1]
# TXT=paste0(texts[], collapse = "\n")
# cat(nchar(TXT))
# texts=read.csv(filecsv,header = FALSE)
head(texts)
texts2 <- texts %>%
#group_by(book) %>%
mutate(linenumber = row_number())%>%
#chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
#                                       ignore_case = TRUE)))) %>%
ungroup()
texts2
library(tidytext)
texts3 <- texts2 %>%
unnest_tokens(word, V1)
texts3
data("stop_words")
cleaned_texts <- texts3 %>%
anti_join(stop_words)
cleaned_texts %>%
count(word, sort = TRUE)
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
cleaned_texts %>%
semi_join(nrcjoy) %>%
count(word, sort = TRUE)
library(tidyr)
bing <- get_sentiments("bing")
review_sentiment <- cleaned_texts %>%
inner_join(bing) %>%
count(word, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
library(ggplot2)
ggplot(review_sentiment, aes(c(1:6),sentiment,fill=sentiment)) +
geom_bar(stat = "identity", show.legend = FALSE)#+
#facet_wrap(~book, ncol = 2, scales = "free_x")
#word_tb <- texts%>%unnest_tokens(word, TXT)
#}
#run_sa('neg_review.csv')
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Web Scraping Project/review_sa")
library(tidyverse)
library(tidytext)
library(stringr)
library(dplyr)
#filecsv='one_pos_review.csv'
#filecsv='one_neg_review.csv'
filecsv='posneg_review.csv'
texts=read.csv(filecsv,header=TRUE, stringsAsFactors = FALSE)
head(texts)
texts$review_num=as.factor(texts$review_num)
### Tag Line Number ###
texts2 <- texts %>%
#group_by(book) %>%
mutate(linenumber = row_number())%>%
ungroup()
texts2
### Tokenize each word (separate each word to a Single Row)
library(tidytext)
texts3 <- texts2 %>%
unnest_tokens(word, review_text)
texts3
###Remove Stop Words
data("stop_words")
cleaned_texts <- texts3 %>%
anti_join(stop_words)
cleaned_texts %>%
count(word, sort = TRUE)
###Retrieve 'Joy' sentiments and store it in nrcjoy
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
### List out Words associated with Joy and their Counts
#cleaned_texts %>%
#  semi_join(nrcjoy) %>%
#  count(word, sort = TRUE)
### Find a sentiment score for each word using the Bing lexicon
library(tidyr)
bing <- get_sentiments("bing")
review_sentiment <- cleaned_texts %>%
inner_join(bing) %>%
count(review_num, index = linenumber,
sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
### Finally, plot the trajectory of sentiments as we go along each word
library(ggplot2)
ggplot(review_sentiment, aes(index,sentiment,fill=(sentiment>0))) +
geom_bar(stat = "identity", show.legend = FALSE) +
xlab('Review Number') +
ylab('Sentiment') +
facet_wrap(~review_num, ncol = 2, scales = "free_x") +
theme_dark() +
ggtitle('Sentiment Analysis on Negative and Positive Reviews')
#word_tb <- texts%>%unnest_tokens(word, TXT)
#}
#run_sa('neg_review.csv')
#ggplot(cleaned_texts, aes(index,sentiment,fill=sentiment)) +
#  geom_bar(stat = "identity", show.legend = FALSE)
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Web Scraping Project/review_sa")
library(tidyverse)
library(tidytext)
library(stringr)
library(dplyr)
#filecsv='one_pos_review.csv'
#filecsv='one_neg_review.csv'
filecsv='posneg_review.csv'
texts=read.csv(filecsv,header=TRUE, stringsAsFactors = FALSE)
head(texts)
texts$review_num=as.factor(texts$review_num)
### Tag Line Number ###
texts2 <- texts %>%
#group_by(book) %>%
mutate(linenumber = row_number())%>%
ungroup()
texts2
### Tokenize each word (separate each word to a Single Row)
library(tidytext)
texts3 <- texts2 %>%
unnest_tokens(word, review_text)
texts3
###Remove Stop Words
data("stop_words")
cleaned_texts <- texts3 %>%
anti_join(stop_words)
cleaned_texts %>%
count(word, sort = TRUE)
###Retrieve 'Joy' sentiments and store it in nrcjoy
nrcjoy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
### List out Words associated with Joy and their Counts
#cleaned_texts %>%
#  semi_join(nrcjoy) %>%
#  count(word, sort = TRUE)
### Find a sentiment score for each word using the Bing lexicon
library(tidyr)
bing <- get_sentiments("bing")
review_sentiment <- cleaned_texts %>%
inner_join(bing) %>%
count(review_num, index = linenumber,
sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
### Finally, plot the trajectory of sentiments as we go along each word
library(ggplot2)
ggplot(review_sentiment, aes(index,sentiment,fill=(sentiment>0))) +
geom_bar(stat = "identity", show.legend = FALSE) +
xlab('Review Number') +
ylab('Sentiment') +
facet_wrap(~review_num, ncol = 2, scales = "free_x") +
theme_dark() +
ggtitle('Sentiment Analysis on Negative and Positive Reviews')
#word_tb <- texts%>%unnest_tokens(word, TXT)
#}
#run_sa('neg_review.csv')
#ggplot(cleaned_texts, aes(index,sentiment,fill=sentiment)) +
#  geom_bar(stat = "identity", show.legend = FALSE)
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Web Scraping Project/review_sa")
library(tidyverse)
library(tidytext)
library(stringr)
library(dplyr)
###Read Positive & Negative Reviews###
filecsv='posneg_review.csv'
texts=read.csv(filecsv,header=TRUE, stringsAsFactors = FALSE)
head(texts)
texts$review_num=as.factor(texts$review_num)
### Tag Line Number ###
texts2 <- texts %>%
mutate(linenumber = row_number())%>%
ungroup()
texts2
### Tokenize each word (separate each word to a Single Row)
library(tidytext)
texts3 <- texts2 %>%
unnest_tokens(word, review_text)
texts3
###Remove Stop Words
data("stop_words")
cleaned_texts <- texts3 %>%
anti_join(stop_words)
cleaned_texts %>%
count(word, sort = TRUE)
### Find a sentiment score for each word using the Bing lexicon
library(tidyr)
bing <- get_sentiments("bing")
review_sentiment <- cleaned_texts %>%
inner_join(bing) %>%
count(review_num, index = linenumber,
sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
### Finally, plot the trajectory of sentiments as we go along each word
library(ggplot2)
ggplot(review_sentiment, aes(index,sentiment,fill=(sentiment>0))) +
geom_bar(stat = "identity", show.legend = FALSE) +
xlab('Review Number') +
ylab('Sentiment') +
facet_wrap(~review_num, ncol = 2, scales = "free_x") +
theme_dark() +
ggtitle('Sentiment Analysis on Negative and Positive Reviews')
