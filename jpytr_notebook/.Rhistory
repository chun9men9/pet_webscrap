waffle_list
waffle(rows = 4, waffle_list)
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
A=c('a','b','c')
B=c(1,2,3)
paste0(A,B)
paste0(A,B,sep=""")
)
paste0(A,B,sep="\"")
paste0(A,B,sep='"')
'"'*3
rep('"',3)
paste0(B,A,B,sep='"')
paste0('"',as.character(boro_waffle_df$crime_level),'"')
crime_lbl=paste0('"',as.character(boro_waffle_df$crime_level),'"')
crime_lbl
length(crime_lbl)
length(crime_lbl[1])
nchar(crime_lbl[1])
paste(crime_lbl,boro_waffle_df$n_nbhood)
paste(crime_lbl,boro_waffle_df$n_nbhood, sep='=')
paste(crime_lbl,boro_waffle_df$n_nbhood, sep='=', collapse = ',')
crime_vec=paste(crime_lbl,boro_waffle_df$n_nbhood, sep='=', collapse = ',')
crime_vec
waffle(rows=4,crime_vec)
crime_vec=paste(crime_lbl,boro_waffle_df$n_nbhood, sep='=')
waffle(rows=4,crime_vec)
crime_vec=paste(boro_waffle_df$crime_level,boro_waffle_df$n_nbhood, sep='=')
waffle(rows=4,crime_vec)
waffle(rows=4,boro_waffle_df$n_nbhood)
?geom_tile
df <- data.frame(
x = rep(c(2, 5, 7, 9, 12), 2),
y = rep(c(1, 2), each = 5),
z = factor(rep(1:5, each = 2)),
w = rep(diff(c(0, 4, 6, 8, 10, 14)), 2)
)
ggplot(df, aes(x, y)) +
geom_tile(aes(fill = z))
airbnb_waffle_df
c('Very Low'=count_level('Very Low'))
count_level=function(level){
return(nrow(airbnb_waffle_df==level))
}
c('Very Low'=count_level('Very Low'))
c('Very Low'=count_level('High'))
count_level=function(level){
return(nrow(airbnb_waffle_df%>%filter(crime_level==level)))
}
c('Very Low'=count_level('Very Low'))
c('Very Low'=count_level('High'))
count_level=function(boro,level){
return(nrow(airbnb_waffle_df%>%filter(boro==boro,crime_level==level)))
}
c('Very Low'=count_level('Staten Island','High'))
manhattan_waffle=airbnb_waffle_df%>%filter(boro=='Manhattan')%>%select(crime_level,n_nbhood)
#waffle(rows=4, bronx_waffle$n_nbhood)
waffle(rows=4,manhattan_waffle$n_nbhood) + scale_fill_gradient()
manhattan_waffle=airbnb_waffle_df%>%filter(boro=='Manhattan')%>%select(crime_level,n_nbhood)
#waffle(rows=4, bronx_waffle$n_nbhood)
waffle(rows=4,manhattan_waffle$n_nbhood)
manhattan_waffle=airbnb_waffle_df%>%filter(boro=='Staten Island')%>%select(crime_level,n_nbhood)
#waffle(rows=4, bronx_waffle$n_nbhood)
waffle(rows=4,manhattan_waffle$n_nbhood)
manhattan_waffle
airbnb_waffle_df%>%filter(boro=='Staten Island',crime_level=='High')
count_level=function(boro,level){
return(nrow(airbnb_waffle_df%>%filter(boro==boro,crime_level==level))[0])
}
c('Very Low'=count_level('Very Low'))
c('Very Low'=count_level('Staten Island','High'))
shiny::runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
shiny::runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
?addPolylines
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
View(subway_df)
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
?makeIcon
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/Desktop/Airbnb_shinyapp-master')
runApp('C:/Users/cmlim/Desktop/Airbnb_shinyapp-master')
runApp('C:/Users/cmlim/Desktop/Airbnb_shinyapp-master')
shiny::runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/Desktop/Airbnb_shinyapp-master')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
?groupColors
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
?airbnb_map_df
View(airbnb_map_df)
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
?round
library(dplyr)
library(geosphere)
load('subway_df.Rda')
load('airbnb_map_df.Rda')
subway_id=c()
subway_dist=c()
for (rownum in 1:nrow(airbnb_map_df)){
#Loop through all Airbnb Rows(Listings)
#Find distance of each Airbnb Listing at rownum to all Subway Stations
alldist=distHaversine(p1=airbnb_map_df[rownum,c('long','lat')], p2=subway_df[,c('long','lat')])
#Save Subway ID of Nearest Station into 'id' column of Airbnb Data Frame
subway_id[rownum]=subway_df[which(alldist==min(alldist)),'subway_id']
#Save Distance of Nearest Station into 'subway_dist' column of Airbnb Data Frame
subway_dist[rownum]=min(alldist)
}
airbnb_map_df$subway_id=subway_id
airbnb_map_df$subway_dist=round(subway_dist,digits=0) #Round to the nearest meter
airbnb_map_df=inner_join(airbnb_map_df,subway_df[,c('subway_name','subway_line','subway_id')],by='subway_id')
save(airbnb_map_df, file='airbnb_map_df.Rda')
load(airbnb_map_df.Rda)
load('airbnb_map_df.Rda')
View(airbnb_map_df)
load('../../../../Desktop/Data Vis Shiny App/Airbnb/airbnb_map_df.Rda')
load('../../../../Desktop/Data Vis Shiny App/Airbnb/airbnb_map_df.Rda')
View(airbnb_map_df)
save(airbnb_map_df, file='airbnb_map_df.Rda')
load('airbnb_map_df.Rda')
round(airbnb_map_df$subway_dist)
airbnb_map_df$subway_dist=round(airbnb_map_df$subway_dist)
save(airbnb_map_df, file='airbnb_map_df.Rda')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
pwd
getwd()
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
View(airbnb_nbhood_map)
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
getwd()
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
?addMarkers
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
shiny::runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
runApp('C:/Users/cmlim/git_proj/airbnb_rshiny')
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
aggr(sleep) #A graphical interpretation of the missing values and their
#combinations within the dataset.
library(mice) #Load the multivariate imputation by chained equations library.
md.pattern(sleep) #Can also view this information from a data perspective.
install.packages('VIM')
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
aggr(sleep) #A graphical interpretation of the missing values and their
#combinations within the dataset.
library(mice) #Load the multivariate imputation by chained equations library.
md.pattern(sleep) #Can also view this information from a data perspective.
install.packages('mice')
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
aggr(sleep) #A graphical interpretation of the missing values and their
#combinations within the dataset.
library(mice) #Load the multivariate imputation by chained equations library.
md.pattern(sleep) #Can also view this information from a data perspective.
?sleep
sapply(sleep, sd, na.rm=TRUE)
View(sleep)
aggr(sleep)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
?cor
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
missing.data
install.packages('caret')
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = "medianImpute")
class(pre)
missing.data
pre = preProcess(missing.data, method = "medianImpute")
class(pre)
missing.data = predict(pre, missing.data)
missing.data
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
pre = preProcess(missing.data, method = c("scale", "medianImpute"))
missing.data = predict(pre, missing.data)
missing.data
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
install.packages('Hmisc')
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
install.packages('RANN')
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
imputed.1nn
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
imputed.5nn
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
pre.5nn = preProcess(missing.data, method = 'knnImpute', k=5)
imputed.5nn = predict(pre.5nn, missing.data)
imputed.5nn
?impute
?predict
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd, na.rm=TRUE) #Standard deviations for the sleep dataset; any issues?
aggr(sleep) #A graphical interpretation of the missing values and their
#combinations within the dataset.
library(mice) #Load the multivariate imputation by chained equations library.
md.pattern(sleep) #Can also view this information from a data perspective.
library(caret)
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
#Imputing using 1NN.
imputed.1nn = kNN(missing.data, k=1)
imputed.1nn
#Imputing using 5NN.
imputed.5nn = kNN(missing.data, k=5)
imputed.5nn
#Imputing using 9NN.
imputed.9nn = kNN(missing.data, k=9)
imputed.9nn
### Imputing with caret
### Note: knnImpute with caret::preProcess force normalization
#Imputing using 1NN.
pre.1nn = preProcess(missing.data, method = 'knnImpute', k=1)
imputed.1nn = predict(pre.1nn, missing.data)
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#sepal measurements and the flower species.
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the c
install.packages('deldir')
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
table(iris$Species, iris.imputed1NN$Species)
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#sepal measurements and the flower species.
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
#mark the missing values.
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
iris.imputed12NN = kNN(iris.example, k = 12)
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
install.packages('kknn')
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Web Scraping Project/jpytr_notebook")
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
gen_wordcloud=function(filecsv, cloudname,excludelis=c('')){
texts=read.csv(filecsv,header = FALSE)
head(texts)
texts=texts[,2]
pos_texts=paste0(texts[], collapse = "\n")
nchar(pos_texts)
docs <- Corpus(VectorSource(texts))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
default_rmv=c("dog","dogs","food","foods",'can','canned')
docs <- tm_map(docs, removeWords, c(default_rmv,excludelis) )
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
pdf(paste0(cloudname,'.pdf'))
wordcloud(words = d$word, freq = d$freq, min.freq = 1, scale=c(2.5,0.2),use.r.layout=FALSE,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dev.off()
}
#benefits_exclude=c('artificial')
neg_exclude=c('good','eat','free','now','likes','one','very','great','love','loves','loved','like','always','absolutely','eatone')
pos_exclude=c('dry','old','one')
neg_title_exclude=c('love','much','quality','cans','products','product','eat',',much','great','good','like','likes','picky')
gen_wordcloud('benefits.csv','benefits')#,benefits_exclude)
gen_wordcloud('neg_review.csv','neg_reviews',neg_exclude)
gen_wordcloud('pos_review.csv','pos_reviews',pos_exclude)
gen_wordcloud('neg_title.csv','neg_titles',neg_title_exclude)
gen_wordcloud('pos_title.csv','pos_titles')
neg_exclude=c('cans','will','good','eat','free','now','likes','one','very','great','love','loves','loved','like','always','absolutely','eatone')
gen_wordcloud('neg_review.csv','neg_reviews',neg_exclude)
neg_exclude=c('product', 'cans','will','good','eat','free','now','likes','one','very','great','love','loves','loved','like','always','absolutely','eatone')
gen_wordcloud('neg_review.csv','neg_reviews',neg_exclude)
neg_exclude=c('product', 'cans','will','good','eat',
'free','now','likes','one','very','great','just',
'love','loves','loved','like','always','absolutely','eatone')
read.csv('neg_review.csv')
neg_exclude=c('product', 'cans','will','good','eat',
'free','now','likes','one','very','great','just',
'love','loves','loved','like','always','absolutely','eatone')
gen_wordcloud('neg_review.csv','neg_reviews',neg_exclude)
gen_wordcloud('neg_review.csv','neg_reviews',neg_exclude)
neg_exclude=c('product', 'cans','will','good','eat','chicken',
'free','now','likes','one','very','great','just',
'love','loves','loved','like','always','absolutely','eatone')
gen_wordcloud('neg_review.csv','neg_reviews',neg_exclude)
